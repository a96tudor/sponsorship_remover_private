/**
 * @author Micah Price (98mprice@gmail.com)
 * Javascript implementation of ../python/predict.py
 *
 * Uses tensorflow.js to read in converted Keras model and
 * run predictions on it.
 *
 * File dependencies:
 * - Generated word index at ../../output/misc/word_index.json
 * - Trained model uploaded to sponsorship_remover_temp_model/master/js
 * - Dataset at ../../dataset/data.csv
 */

require('@tensorflow/tfjs-node')
const tf = require('@tensorflow/tfjs')
global.fetch = require('node-fetch')
const fs = require('fs')
const nj = require('numjs')
const pb = require('progress')

var wordIndex = require('../../output/misc/word_index.json');

const numWords = 5000
const embedDim = 64
const batchSize = 32

const bar = new pb(':elapseds [:bar] :busyWith...', {
  total: 8, width: 35
})

/**
 * Since tf.js doesn't have a built in tokenizer, I had to write my own.
 * Very basic, but it seems to work well enough
 */
class Tokenizer {

  /**
   * The vocabulary is just a object where every word (key) is assigned a unique
   * integer token (value) to represent it.
   */
  constructor(vocabulary) {
    this.vocabulary = vocabulary
  }

  /**
   * Fit on texts creates a vector comprised of the token values
   * of each string in the vocabulary.
   *
   * TODO: Ideal solution would be to use this function, but
   * unfortunately I'm having a problem where strings that occour
   * with the same frequency don't seem to match the same integer token as
   * assigned by the default Keras Tokenizer class.
   * I can't figure out how equal frequency strings are being sorted,
   * so until then I'm just loading in the word_index dict (generated by
   * Keras) directly.
   *
   * The reason why this function would be useful to have is because dynamically
   * generating this vector would be needed if we were to train the model with tfjs.
   * For now, since we're just predicting based on a preset model, it's not
   * important.
   */
  fitOnTexts(texts) {
    bar.tick({
      'busyWith': 'Finding unique words'
    })
    const words = texts.reduce((acc, text) => [...acc,
      ...text.split(' ')], [])

    const frequencyWords = {}
    for (const word of words) {
      if (!frequencyWords[word])
        frequencyWords[word] = 0
      frequencyWords[word] += 1
    }

    const sortedFrequencyWords = []
    for (const key of Object.keys(frequencyWords)) {
      sortedFrequencyWords.push({
        word: key,
        count: frequencyWords[key]
      })
    }

    sortedFrequencyWords.sort((a, b) => b.count - a.count)
    const uniqueWords = sortedFrequencyWords.map(obj => obj.word)

    bar.tick({
      'busyWith': 'Creating a word vocabulary'
    })
    this.vocabulary = uniqueWords.reduce((acc, word, idx) => ({
       ...acc, [word]: idx + 1}), {})
  }

  /**
   * Vocabulary.
   */
  get wordIndex() {
    return this.vocabulary
  }

  /**
   * Gets the top numWords from vocabulary.
   * @return {array} Array of strings of length numWords.
   */
  get topWords() {
    const topWords = []
    for (const [idx, key] of Object.keys(this.vocabulary).entries()) {
      if (idx >= numWords) break
      topWords.push(key)
    }
    return topWords
  }

  /**
   * Transforms each text in texts to a sequence of integer tokens,
   * bounded by numWords.
   * @param {string} text Containing csv file contents.
   * @return {array} Array of texts converted to tokens.
   */
  textsToSequences(texts) {
    const words = texts.reduce((acc, text) => [...acc,
      ...text.split(' ')], [])
    const topWords = this.topWords
    return texts.map(text => text.split(' ').map(word =>
      topWords.includes(word) ? this.vocabulary[word] : null).filter(num => num != null))
  }

}

/**
 * Reads data from csv file and stores the first 2 columns
 * into xText and yText respectively.
 * @param {string} text Containing csv file contents.
 * @return {object} Object with both features and targets.
 */
function readData(text) {
  const lines = text.split('\n')
  lines.shift()
  const xText = lines.map(line => line.split(',')[0])
  const yText = lines.map(line => line.split(',')[1])
  xText.pop()
  yText.pop()
  return { xText: xText, yText: yText }
}

/**
 * Feature length is used for padding/ truncating (ensuring
 * that each sequence is of the same length in batch).
 * Max tokens set to avg + 2 std dev.
 * @param {array} xTokens Word embeddings, used to calculate feature length.
 * @return {number} Max tokens.
 */
function getFeatureLength(xTokens) {
  numTokens = nj.array(xTokens.map(tokens => tokens.length))
  // floor required because js and python seem to handle floating point arithmatic differently
  avgTokens = Math.floor(numTokens.mean())
  return Math.floor(avgTokens + 2 * numTokens.std())
}

/**
 * Padding/ truncating to ensure that each sequence is of
 * the same length in batch. 'Pre' padding, 'post' truncating:
 * Zeros added at beginning because this prevents early fatigue of network.
 * @param {array} xTokens Word embeddings, used to calculate feature length.
 * @param {number} maxLength Max feature length.
 * @return {array} Padded/ truncated xTokens.
 */
function padSequences(xTokens, maxLength) {
  const xPad = []
  for (const xTokenArr of xTokens) {
    if ((maxLength - xTokenArr.length) > 0)  {
      // pad
      xPad.push([...new Array(maxLength - xTokenArr.length).fill(0),
        ...xTokenArr])
    } else {
      // truncate
      xPad.push(xTokenArr.slice(0, maxLength))
    }
  }
  return xPad
}

/**
 *  Loads model and runs predictions on it
 */
async function createModel(text) {

  bar.tick({ 'busyWith': 'Reading data' })
  let { xText, yText } = readData(text)
  const alphaWords = []
  for (let sentence of xText) {
    alphaWords.push(sentence.split(' ').filter(word => /^[a-z]+$/.test(word)).join(' '))
  }
  xText = alphaWords

  bar.tick({ 'busyWith': 'Initialising Tokenizer' })
  const tokenizer = new Tokenizer(wordIndex)
  bar.tick({ 'busyWith': 'Fitting text' })

  //tokenizer.fitOnTexts(xText)

  // preprocess features
  bar.tick({ 'busyWith': 'Converting text to sequences' })
  const xTokens = tokenizer.textsToSequences(xText)
  const featureLength = getFeatureLength(xTokens)

  bar.tick({ 'busyWith': 'Padding/ truncating sequences' })
  const xPad = padSequences(xTokens, featureLength)

  bar.tick({ 'busyWith': 'Loading Model' })
  const model = await tf.loadModel('https://raw.githubusercontent.com/micah5/sponsorship_remover_temp_model/master/js/model.json')

  bar.tick({ 'busyWith': 'Prediction' })
  xTestSponsored = ['mecha fights giant robots youre either in or youre in and best of all you guys can get an ad free 30-day trial a verve',
                    'premium for people what does that mean well that includes offline viewing you can watch all your favorite shows on the',
                    'go without internet access and of course it also means no ads need i say more so to start your ad free 30-day trial',
                    'verve premium just go to verve co slash jacksfilms link in the description again thats verve co slash jacksfilms',
                    'for a 30-day ad free trial a verve premium oh i have a challenge for you lets create a flag for gamers subscribe',
                    'for more leaks also click right here to see the preview see i episode uh heres a clip why oh why should you go vegan to',
                    'this video was made possible by brilliant']
  xTestNotSponsored = [ 'sometimes submarines sink their systems fail and nobody can get to them before oxygen runs out. as submarines become better at masking themselves submarine tracking technology is simultaneously',
                        'many submarine operating countries have rescue submarines that can hypothetically be used to save stranded submariners by going down latching on and shuttling sailors to the surface but in practice these have never really had much action',
                        '24 hours after the last reading these will drift to only about 1.15 miles or 1.85 kilometers of accuracy. now this technique combined with the consultation of maps is usually fine since most of the',
                        'and some separate systems designed for use when the main systems are compromised but vlf radio forms the bulk of communications with most submarines. but the fact that submarines spend their time underwater in stealth also makes another crucial',
                        'providers like hp and dell you can often run into proprietary parts limited upgrade ability and configurability and',
                        'questionable software load outs that can affect the overall experience so the popularity of machines that are',
                        'built from retail parts by smaller system integrators like ibuypower or main gear is understandable boutique']
  xTest = [...xTestSponsored, ...xTestNotSponsored]
  const xTestTokens = tokenizer.textsToSequences(xTest)
  const xTestPad = padSequences(xTestTokens, featureLength)
  const xTestTensor = tf.tensor2d(xTestPad)
  //xTestTensor.print(true)
  const prediction = model.predict(xTestTensor)
  const outputData = await prediction.dataSync()
  bar.tick({ 'busyWith': 'Done' })
  console.log('\n', outputData)
  console.log('Sponsored accuracy:', outputData.slice(0,
    xTestSponsored.length).filter(x => x < 0.5).length/xTestSponsored.length)
  console.log('Not Sponsored accuracy:', outputData.slice(xTestSponsored.length,
    xTest.length).filter(x => x >= 0.5).length/xTestNotSponsored.length)
}

fs.readFile('./dataset/data.csv', 'utf8', (error, data) => {
    if (error) throw error
    createModel(data.toString())
})
