NUM_WORDS = 2000 # for entire vocabulary use len(tokenizer.word_index)
EMBED_DIM = 128
BATCH_SIZE = 32
EPOCHS = 5
